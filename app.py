import streamlit as st
import pandas as pd
import docx2txt
import pdfplumber
import re
import google.generativeai as genai

# --- C·∫§U H√åNH TRANG ---
st.set_page_config(page_title="Tr·ª£ l√Ω Gi√°o √Ån NLS (Gemini)", page_icon="‚ú®")

# --- 1. T·ª™ ƒêI·ªÇN D·ªÆ LI·ªÜU ---
SUBJECT_MAPPING = {
    "To√°n h·ªçc": {"keywords": ["geogebra", "m√°y t√≠nh c·∫ßm tay", "excel", "b·∫£ng t√≠nh", "ƒë·ªì th·ªã", "m√¥ ph·ªèng"], "default_id": "5.1TC1a"},
    "Ng·ªØ vƒÉn": {"keywords": ["so·∫°n th·∫£o", "word", "powerpoint", "slide", "video", "clip", "tra c·ª©u", "s√¢n kh·∫•u h√≥a"], "default_id": "3.1TC1a"},
    "Ti·∫øng Anh": {"keywords": ["t·ª´ ƒëi·ªÉn", "app", "file nghe", "audio", "video", "ghi √¢m", "l·ªìng ti·∫øng", "chat"], "default_id": "2.1TC1a"},
    "KHTN (L√Ω/H√≥a/Sinh)": {"keywords": ["th√≠ nghi·ªám ·∫£o", "phet", "m√¥ ph·ªèng", "s·ªë li·ªáu", "k√≠nh hi·ªÉn vi"], "default_id": "1.2TC1a"},
    "L·ªãch s·ª≠ & ƒê·ªãa l√Ω": {"keywords": ["b·∫£n ƒë·ªì s·ªë", "google earth", "l∆∞·ª£c ƒë·ªì", "tranh ·∫£nh", "gps", "t∆∞ li·ªáu", "internet"], "default_id": "1.1TC1a"},
    "Tin h·ªçc": {"keywords": ["l·∫≠p tr√¨nh", "code", "thu·∫≠t to√°n", "m√°y t√≠nh", "ph·∫ßn m·ªÅm", "internet", "th∆∞ m·ª•c"], "default_id": "5.4TC1a"},
    "C√¥ng ngh·ªá": {"keywords": ["b·∫£n v·∫Ω", "thi·∫øt k·∫ø", "cad", "m√¥ h√¨nh", "video h∆∞·ªõng d·∫´n", "quy tr√¨nh"], "default_id": "3.1TC1b"},
    "Hƒê Tr·∫£i nghi·ªám": {"keywords": ["kh·∫£o s√°t", "google form", "canva", "poster", "video", "·∫£nh", "thuy·∫øt tr√¨nh"], "default_id": "2.2TC1a"},
    "Ngh·ªá thu·∫≠t": {"keywords": ["v·∫Ω m√°y", "ch·ªânh ·∫£nh", "video", "ghi √¢m", "nh·∫°c c·ª• ·∫£o"], "default_id": "3.1TC1a"},
    "GDTC": {"keywords": ["video", "ƒë·ªìng h·ªì b·∫•m gi·ªù", "nh·ªãp tim", "app s·ª©c kh·ªèe", "ghi h√¨nh"], "default_id": "4.3TC1a"}
}

# --- 2. H√ÄM G·ªåI GEMINI (C√ì D·ª∞ PH√íNG) ---
def ask_gemini_auto(lesson_text, subject, nls_content):
    try:
        # 1. Th·ª≠ l·∫•y Key t·ª´ h·ªá th·ªëng
        api_key = st.secrets.get("GEMINI_API_KEY", None)
        
        # 2. N·∫øu kh√¥ng c√≥ Key -> Tr·∫£ v·ªÅ None ƒë·ªÉ d√πng M·∫´u c√¢u
        if not api_key: return None
        
        # 3. N·∫øu c√≥ Key -> G·ªçi AI
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        prompt = f"""
        ƒê√≥ng vai chuy√™n gia gi√°o d·ª•c.
        B√†i h·ªçc m√¥n: {subject}. T√≥m t·∫Øt: "{lesson_text[:1000]}".
        NƒÉng l·ª±c s·ªë: "{nls_content}".
        
        H√£y ƒë·ªÅ xu·∫•t 1 S·∫¢N PH·∫®M S·ªê C·ª§ TH·ªÇ h·ªçc sinh l√†m ƒë∆∞·ª£c.
        Vi·∫øt ng·∫Øn g·ªçn 2-3 c√¢u. M·∫´u: "H·ªçc sinh d√πng [C√¥ng c·ª•] ƒë·ªÉ t·∫°o [S·∫£n ph·∫©m], qua ƒë√≥ [L·ª£i √≠ch]."
        """
        response = model.generate_content(prompt)
        return response.text.strip()
    except:
        return None # N·∫øu l·ªói m·∫°ng ho·∫∑c key sai -> D√πng m·∫´u c√¢u

# --- 3. LOAD DATA & ƒê·ªåC FILE ---
@st.cache_data
def load_nls_data():
    data = {
        'Id': ['1.1TC1a', '1.1TC2b', '1.3TC1a', '2.1TC1a', '2.2TC1a', '3.1TC1a', '3.1TC1b', '4.3TC1a', '5.1TC1a', '5.4TC1a'],
        'Muc': ['TC1', 'TC2', 'TC1', 'TC1', 'TC1', 'TC1', 'TC1', 'TC1', 'TC1', 'TC1'],
        'YCCD': [
            'T√¨m ki·∫øm d·ªØ li·ªáu tr√™n m√¥i tr∆∞·ªùng s·ªë.',
            'ƒê√°nh gi√° ngu·ªìn tin v√† t√¨m ki·∫øm n√¢ng cao.',
            'L∆∞u tr·ªØ v√† qu·∫£n l√Ω d·ªØ li·ªáu khoa h·ªçc.',
            'T∆∞∆°ng t√°c v√† giao ti·∫øp qua c√¥ng ngh·ªá.',
            'Chia s·∫ª th√¥ng tin v√† h·ª£p t√°c nh√≥m.',
            'T·∫°o v√† ch·ªânh s·ª≠a n·ªôi dung s·ªë (vƒÉn b·∫£n, ·∫£nh, video).',
            'T·∫°o s·∫£n ph·∫©m s·ªë ƒë∆°n gi·∫£n th·ªÉ hi·ªán b·∫£n th√¢n.',
            'B·∫£o v·ªá s·ª©c kh·ªèe v√† an to√†n khi d√πng c√¥ng ngh·ªá.',
            'Gi·∫£i quy·∫øt l·ªói k·ªπ thu·∫≠t ƒë∆°n gi·∫£n.',
            'T·ª± h·ªçc v√† c·∫≠p nh·∫≠t ki·∫øn th·ª©c s·ªë.'
        ]
    }
    df = pd.DataFrame(data)
    df_tc2 = df.copy()
    df_tc2['Muc'] = 'TC2'
    return pd.concat([df, df_tc2])

def read_file(uploaded_file):
    try:
        if uploaded_file.name.endswith('.docx'): return docx2txt.process(uploaded_file)
        elif uploaded_file.name.endswith('.pdf'):
            text = ""
            with pdfplumber.open(uploaded_file) as pdf:
                for page in pdf.pages: text += page.extract_text() + "\n"
            return text
    except: return ""

# --- 4. LOGIC PH√ÇN T√çCH ---
def analyze_final(text, df, subject):
    text_lower = text.lower()
    subj_info = SUBJECT_MAPPING.get(subject, {"keywords": [], "default_id": ""})
    
    # T√¨m c√¥ng c·ª•
    found_tools = [kw for kw in subj_info["keywords"] if kw in text_lower]
    if not found_tools: found_tools = ["thi·∫øt b·ªã s·ªë", "internet", "ph·∫ßn m·ªÅm"]

    # T√¨m ID
    matched_ids = []
    for _, row in df.iterrows():
        yccd_words = [w for w in row['YCCD'].lower().split() if len(w) > 4]
        match = sum(1 for w in yccd_words if w in text_lower)
        if (match / len(yccd_words) if yccd_words else 0) > 0.4: matched_ids.append(row)
    
    if not matched_ids and subj_info["default_id"]:
        defs = df[df['Id'] == subj_info["default_id"]]
        if not defs.empty: matched_ids.append(defs.iloc[0])

    # K·∫øt qu·∫£
    results = []
    seen = set()
    for row in matched_ids[:2]:
        if row['Id'] in seen: continue
        seen.add(row['Id'])

        # --- C∆† CH·∫æ TH√îNG MINH ---
        # ∆Øu ti√™n 1: H·ªèi AI (N·∫øu c√≥ Key trong Secrets)
        ai_reply = ask_gemini_auto(text, subject, row['YCCD'])
        
        explanation = ""
        if ai_reply:
            explanation = f"‚ú® **G·ª£i √Ω t·ª´ AI:** {ai_reply}"
        else:
            # ∆Øu ti√™n 2: D√πng M·∫´u c√¢u (N·∫øu kh√¥ng c√≥ Key)
            tools_str = ", ".join(found_tools[:2])
            explanation = (
                f"üìù **G·ª£i √Ω:** H·ªçc sinh s·ª≠ d·ª•ng **{tools_str}** ƒë·ªÉ th·ª±c hi·ªán ho·∫°t ƒë·ªông h·ªçc t·∫≠p. "
                f"S·∫£n ph·∫©m d·ª± ki·∫øn: B√†i tr√¨nh chi·∫øu, Video ho·∫∑c Phi·∫øu h·ªçc t·∫≠p s·ªë. "
                f"Qua ƒë√≥ r√®n luy·ªán k·ªπ nƒÉng '{row['YCCD']}'."
            )

        results.append({
            "id": row['Id'],
            "yccd": row['YCCD'],
            "exp": explanation
        })
    return results

# --- 5. GIAO DI·ªÜN ---
st.title("ü§ñ Gi√°o √Ån NƒÉng L·ª±c S·ªë")
st.caption("H·ªó tr·ª£ gi√°o vi√™n t√¨m nƒÉng l·ª±c s·ªë ph√π h·ª£p trong b√†i d·∫°y.")
st.markdown("---")

col1, col2 = st.columns(2)
grade = col1.selectbox("Kh·ªëi l·ªõp", ["L·ªõp 6", "L·ªõp 7", "L·ªõp 8", "L·ªõp 9"])
subject = col2.selectbox("M√¥n h·ªçc", list(SUBJECT_MAPPING.keys()))
uploaded_file = st.file_uploader("T·∫£i gi√°o √°n (Word/PDF)", type=['docx', 'pdf'])

if uploaded_file and st.button("PH√ÇN T√çCH"):
    target = 'TC1' if grade in ['L·ªõp 6', 'L·ªõp 7'] else 'TC2'
    with st.spinner("ƒêang ph√¢n t√≠ch..."):
        content = read_file(uploaded_file)
        if len(content) < 50: st.warning("Kh√¥ng t√¨m th·∫•y NƒÉng l·ª±c s·ªë")
        else:
            df = load_nls_data()
            res = analyze_final(content, df[df['Muc'] == target], subject)
            
            st.divider()
            if res:
                st.success(f"‚úÖ T√¨m th·∫•y {len(res)} ƒë·ªÅ xu·∫•t!")
                for item in res:
                    with st.expander(f"üìå M√£: {item['id']}", expanded=True):
                        st.code(f"{item['id']}: {item['yccd']}", language="text")
                        st.info(item['exp'])
            else:
                st.warning("Kh√¥ng t√¨m th·∫•y NƒÉng l·ª±c s·ªë cho b√†i h·ªçc n√†y")