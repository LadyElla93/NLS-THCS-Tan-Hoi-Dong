import streamlit as st
import pandas as pd
import docx2txt
import pdfplumber
import re

# --- C·∫§U H√åNH ---
st.set_page_config(page_title="Tr·ª£ l√Ω NLS (Deep Scan)", page_icon="üéØ")

# --- 1. T·ª™ ƒêI·ªÇN M√îN H·ªåC & S·∫¢N PH·∫®M ---
# C·∫•u tr√∫c: keywords (ƒë·ªÉ t√¨m), map_id (NLS t∆∞∆°ng ·ª©ng), product (s·∫£n ph·∫©m g·ª£i √Ω)
SUBJECT_DATA = {
    "To√°n h·ªçc": {
        "kw": ["geogebra", "m√°y t√≠nh c·∫ßm tay", "excel", "ƒë·ªì th·ªã", "m√¥ ph·ªèng", "t√≠nh to√°n"],
        "id": "5.1TC1a", "prod": "K·∫øt qu·∫£ t√≠nh to√°n/H√¨nh v·∫Ω ƒë·ªì th·ªã s·ªë"
    },
    "Ng·ªØ vƒÉn": {
        "kw": ["so·∫°n th·∫£o", "word", "powerpoint", "tr√¨nh chi·∫øu", "video", "clip", "tra c·ª©u", "s√¢n kh·∫•u h√≥a", "tranh ·∫£nh"],
        "id": "3.1TC1a", "prod": "Slide thuy·∫øt tr√¨nh ho·∫∑c VƒÉn b·∫£n s·ªë h√≥a"
    },
    "Ti·∫øng Anh": {
        "kw": ["t·ª´ ƒëi·ªÉn", "app", "file nghe", "audio", "video", "ghi √¢m", "loa", "internet"],
        "id": "2.1TC1a", "prod": "File ghi √¢m ho·∫∑c H·ªôi tho·∫°i qua ·ª©ng d·ª•ng"
    },
    "KHTN": {
        "kw": ["th√≠ nghi·ªám ·∫£o", "phet", "m√¥ ph·ªèng", "s·ªë li·ªáu", "k√≠nh hi·ªÉn vi", "video th√≠ nghi·ªám"],
        "id": "1.2TC1a", "prod": "B·∫£ng s·ªë li·ªáu ho·∫∑c Video m√¥ ph·ªèng th√≠ nghi·ªám"
    },
    "L·ªãch s·ª≠ & ƒê·ªãa l√Ω": {
        "kw": ["b·∫£n ƒë·ªì", "l∆∞·ª£c ƒë·ªì", "google earth", "tranh ·∫£nh", "gps", "t∆∞ li·ªáu", "internet"],
        "id": "1.1TC1a", "prod": "B·∫£n ƒë·ªì s·ªë ho·∫∑c B·ªô s∆∞u t·∫≠p t∆∞ li·ªáu ·∫£nh"
    },
    "Tin h·ªçc": {
        "kw": ["l·∫≠p tr√¨nh", "code", "thu·∫≠t to√°n", "m√°y t√≠nh", "ph·∫ßn m·ªÅm", "th∆∞ m·ª•c", "t·ªáp"],
        "id": "5.4TC1a", "prod": "Ch∆∞∆°ng tr√¨nh m√°y t√≠nh ho·∫∑c C·∫•u tr√∫c th∆∞ m·ª•c"
    },
    "C√¥ng ngh·ªá": {
        "kw": ["b·∫£n v·∫Ω", "thi·∫øt k·∫ø", "cad", "m√¥ h√¨nh", "video", "quy tr√¨nh"],
        "id": "3.1TC1b", "prod": "B·∫£n thi·∫øt k·∫ø k·ªπ thu·∫≠t s·ªë"
    },
    "Hƒê Tr·∫£i nghi·ªám": {
        "kw": ["kh·∫£o s√°t", "form", "canva", "poster", "video", "·∫£nh", "thuy·∫øt tr√¨nh"],
        "id": "2.2TC1a", "prod": "Poster truy·ªÅn th√¥ng ho·∫∑c K·∫øt qu·∫£ kh·∫£o s√°t online"
    },
    "Ngh·ªá thu·∫≠t": {
        "kw": ["v·∫Ω", "ch·ªânh ·∫£nh", "video", "ghi √¢m", "nh·∫°c c·ª•"],
        "id": "3.1TC1a", "prod": "T√°c ph·∫©m tranh/nh·∫°c s·ªë"
    },
    "GDTC": {
        "kw": ["video", "ƒë·ªìng h·ªì", "nh·ªãp tim", "app", "ghi h√¨nh"],
        "id": "4.3TC1a", "prod": "Video ph√¢n t√≠ch ƒë·ªông t√°c"
    }
}

# --- 2. H√ÄM ƒê·ªåC D·ªÆ LI·ªÜU CHU·∫®N ---
@st.cache_data
def load_nls_db():
    # D·ªØ li·ªáu NLS c·ªët l√µi
    data = [
        {"Id": "1.1TC1a", "Muc": "TC1", "YCCD": "X√°c ƒë·ªãnh nhu c·∫ßu v√† t√¨m ki·∫øm d·ªØ li·ªáu tr√™n m√¥i tr∆∞·ªùng s·ªë."},
        {"Id": "1.1TC2b", "Muc": "TC2", "YCCD": "T·ªï ch·ª©c t√¨m ki·∫øm th√¥ng tin n√¢ng cao, ph√¢n lo·∫°i k·∫øt qu·∫£."},
        {"Id": "1.2TC1a", "Muc": "TC1", "YCCD": "Ph√¢n t√≠ch v√† ƒë√°nh gi√° d·ªØ li·ªáu, th√¥ng tin s·ªë."},
        {"Id": "2.1TC1a", "Muc": "TC1", "YCCD": "S·ª≠ d·ª•ng c√¥ng ngh·ªá ƒë·ªÉ t∆∞∆°ng t√°c v√† giao ti·∫øp ph√π h·ª£p."},
        {"Id": "2.2TC1a", "Muc": "TC1", "YCCD": "Chia s·∫ª th√¥ng tin v√† ph·ªëi h·ª£p qua m√¥i tr∆∞·ªùng s·ªë."},
        {"Id": "3.1TC1a", "Muc": "TC1", "YCCD": "T·∫°o v√† bi√™n t·∫≠p n·ªôi dung s·ªë (vƒÉn b·∫£n, h√¨nh ·∫£nh, √¢m thanh)."},
        {"Id": "4.3TC1a", "Muc": "TC1", "YCCD": "S·ª≠ d·ª•ng thi·∫øt b·ªã s·ªë an to√†n, b·∫£o v·ªá s·ª©c kh·ªèe."},
        {"Id": "5.1TC1a", "Muc": "TC1", "YCCD": "Gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ k·ªπ thu·∫≠t c∆° b·∫£n c·ªßa thi·∫øt b·ªã s·ªë."},
        {"Id": "5.4TC1a", "Muc": "TC1", "YCCD": "T·ª± c·∫≠p nh·∫≠t v√† ph√°t tri·ªÉn nƒÉng l·ª±c s·ªë b·∫£n th√¢n."}
    ]
    # Nh√¢n b·∫£n TC1 th√†nh TC2 cho demo
    full_data = []
    for item in data:
        full_data.append(item)
        item2 = item.copy()
        item2["Muc"] = "TC2"
        full_data.append(item2)
    return pd.DataFrame(full_data)

# --- 3. THU·∫¨T TO√ÅN ƒê·ªåC S√ÇU (DEEP SCAN) ---
def analyze_deep(text, subject, grade):
    results = []
    text_lower = text.lower()
    
    # 1. L·∫•y th√¥ng tin m√¥n h·ªçc
    subj_config = SUBJECT_DATA.get(subject, SUBJECT_DATA["To√°n h·ªçc"])
    keywords = subj_config["kw"]
    
    # 2. C·∫Øt vƒÉn b·∫£n th√†nh c√°c Ho