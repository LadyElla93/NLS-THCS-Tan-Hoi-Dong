import streamlit as st
import pandas as pd
import docx2txt
import pdfplumber
import re
import google.generativeai as genai
from openai import OpenAI # Th∆∞ vi·ªán d√πng ƒë·ªÉ g·ªçi Grok

# --- C·∫§U H√åNH TRANG ---
st.set_page_config(page_title="Tr·ª£ l√Ω Gi√°o √Ån NLS (Multi-AI)", page_icon="üß†")

# --- 1. T·ª™ ƒêI·ªÇN D·ªÆ LI·ªÜU ---
SUBJECT_MAPPING = {
    "To√°n h·ªçc": {"keywords": ["geogebra", "m√°y t√≠nh c·∫ßm tay", "excel", "b·∫£ng t√≠nh", "ƒë·ªì th·ªã", "m√¥ ph·ªèng"], "default_id": "5.1TC1a"},
    "Ng·ªØ vƒÉn": {"keywords": ["so·∫°n th·∫£o", "word", "powerpoint", "slide", "video", "clip", "tra c·ª©u", "s√¢n kh·∫•u h√≥a"], "default_id": "3.1TC1a"},
    "Ti·∫øng Anh": {"keywords": ["t·ª´ ƒëi·ªÉn", "app", "file nghe", "audio", "video", "ghi √¢m", "l·ªìng ti·∫øng", "chat"], "default_id": "2.1TC1a"},
    "KHTN (L√Ω/H√≥a/Sinh)": {"keywords": ["th√≠ nghi·ªám ·∫£o", "phet", "m√¥ ph·ªèng", "s·ªë li·ªáu", "k√≠nh hi·ªÉn vi"], "default_id": "1.2TC1a"},
    "L·ªãch s·ª≠ & ƒê·ªãa l√Ω": {"keywords": ["b·∫£n ƒë·ªì s·ªë", "google earth", "l∆∞·ª£c ƒë·ªì", "tranh ·∫£nh", "gps", "t∆∞ li·ªáu", "internet"], "default_id": "1.1TC1a"},
    "Tin h·ªçc": {"keywords": ["l·∫≠p tr√¨nh", "code", "thu·∫≠t to√°n", "m√°y t√≠nh", "ph·∫ßn m·ªÅm", "internet", "th∆∞ m·ª•c"], "default_id": "5.4TC1a"},
    "C√¥ng ngh·ªá": {"keywords": ["b·∫£n v·∫Ω", "thi·∫øt k·∫ø", "cad", "m√¥ h√¨nh", "video h∆∞·ªõng d·∫´n", "quy tr√¨nh"], "default_id": "3.1TC1b"},
    "Hƒê Tr·∫£i nghi·ªám": {"keywords": ["kh·∫£o s√°t", "google form", "canva", "poster", "video", "·∫£nh", "thuy·∫øt tr√¨nh"], "default_id": "2.2TC1a"},
    "Ngh·ªá thu·∫≠t": {"keywords": ["v·∫Ω m√°y", "ch·ªânh ·∫£nh", "video", "ghi √¢m", "nh·∫°c c·ª• ·∫£o"], "default_id": "3.1TC1a"},
    "GDTC": {"keywords": ["video", "ƒë·ªìng h·ªì b·∫•m gi·ªù", "nh·ªãp tim", "app s·ª©c kh·ªèe", "ghi h√¨nh"], "default_id": "4.3TC1a"}
}

# --- 2. H√ÄM G·ªåI GEMINI ---
def ask_gemini(text, subject, nls):
    try:
        api_key = st.secrets["GEMINI_API_KEY"]
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
        prompt = f"G·ª£i √Ω 1 s·∫£n ph·∫©m s·ªë c·ª• th·ªÉ h·ªçc sinh l√†m ƒë∆∞·ª£c trong b√†i m√¥n {subject} ƒë·ªÉ ƒë·∫°t nƒÉng l·ª±c '{nls}'. N·ªôi dung b√†i: {text[:1000]}. Ng·∫Øn g·ªçn."
        return model.generate_content(prompt).text.strip()
    except: return None

# --- 3. H√ÄM G·ªåI GROK (M·ªöI) ---
def ask_grok(text, subject, nls):
    try:
        # L·∫•y Key Grok t·ª´ Secrets
        api_key = st.secrets["XAI_API_KEY"]
        client = OpenAI(
            api_key=api_key,
            base_url="https://api.x.ai/v1", # C·ªïng k·∫øt n·ªëi c·ªßa Grok
        )
        
        prompt = f"""
        ƒê√≥ng vai chuy√™n gia EdTech. D·ª±a v√†o b√†i h·ªçc m√¥n {subject} (T√≥m t·∫Øt: "{text[:1000]}..."), v√† nƒÉng l·ª±c s·ªë "{nls}".
        H√£y ƒë·ªÅ xu·∫•t 01 S·∫¢N PH·∫®M S·ªê C·ª§ TH·ªÇ m√† h·ªçc sinh s·∫Ω t·∫°o ra.
        VƒÉn phong: Hi·ªán ƒë·∫°i, ng·∫Øn g·ªçn, t·∫≠p trung v√†o s·∫£n ph·∫©m.
        M·∫´u: "S·∫£n ph·∫©m: [T√™n s·∫£n ph·∫©m]. C√°ch l√†m: [M√¥ t·∫£ ng·∫Øn]."
        """
        
        completion = client.chat.completions.create(
            model="grok-beta", # Ho·∫∑c grok-2 t√πy th·ªùi ƒëi·ªÉm
            messages=[{"role": "system", "content": "You are a helpful education assistant."},
                      {"role": "user", "content": prompt}]
        )
        return completion.choices[0].message.content
    except: return None

# --- 4. H√ÄM LOAD D·ªÆ LI·ªÜU V√Ä ƒê·ªåC FILE (GI·ªÆ NGUY√äN) ---
@st.cache_data
def load_nls_data():
    data = {
        'Id': ['1.1TC1a', '1.1TC2b', '1.3TC1a', '2.1TC1a', '2.2TC1a', '3.1TC1a', '3.1TC1b', '4.3TC1a', '5.1TC1a', '5.4TC1a'],
        'Muc': ['TC1', 'TC2', 'TC1', 'TC1', 'TC1', 'TC1', 'TC1', 'TC1', 'TC1', 'TC1'],
        'YCCD': [
            'T√¨m ki·∫øm d·ªØ li·ªáu tr√™n m√¥i tr∆∞·ªùng s·ªë.',
            'ƒê√°nh gi√° ngu·ªìn tin v√† t√¨m ki·∫øm n√¢ng cao.',
            'L∆∞u tr·ªØ v√† qu·∫£n l√Ω d·ªØ li·ªáu khoa h·ªçc.',
            'T∆∞∆°ng t√°c v√† giao ti·∫øp qua c√¥ng ngh·ªá.',
            'Chia s·∫ª th√¥ng tin v√† h·ª£p t√°c nh√≥m.',
            'T·∫°o v√† ch·ªânh s·ª≠a n·ªôi dung s·ªë (vƒÉn b·∫£n, ·∫£nh, video).',
            'T·∫°o s·∫£n ph·∫©m s·ªë ƒë∆°n gi·∫£n th·ªÉ hi·ªán b·∫£n th√¢n.',
            'B·∫£o v·ªá s·ª©c kh·ªèe v√† an to√†n khi d√πng c√¥ng ngh·ªá.',
            'Gi·∫£i quy·∫øt l·ªói k·ªπ thu·∫≠t ƒë∆°n gi·∫£n.',
            'T·ª± h·ªçc v√† c·∫≠p nh·∫≠t ki·∫øn th·ª©c s·ªë.'
        ]
    }
    df = pd.DataFrame(data)
    df_tc2 = df.copy()
    df_tc2['Muc'] = 'TC2'
    return pd.concat([df, df_tc2])

def read_file(uploaded_file):
    try:
        if uploaded_file.name.endswith('.docx'): return docx2txt.process(uploaded_file)
        elif uploaded_file.name.endswith('.pdf'):
            text = ""
            with pdfplumber.open(uploaded_file) as pdf:
                for page in pdf.pages: text += page.extract_text() + "\n"
            return text
    except: return ""

# --- 5. LOGIC PH√ÇN T√çCH T·ªîNG H·ª¢P ---
def analyze_mixed_ai(text, df, subject):
    text_lower = text.lower()
    subj_info = SUBJECT_MAPPING.get(subject, {"keywords": [], "default_id": ""})
    
    # T√¨m c√¥ng c·ª•
    found_tools = [kw for kw in subj_info["keywords"] if kw in text_lower]
    if not found_tools: found_tools = ["c√¥ng ngh·ªá", "m√°y t√≠nh", "internet"]

    # T√¨m ID
    matched_ids = []
    for _, row in df.iterrows():
        yccd_words = [w for w in row['YCCD'].lower().split() if len(w) > 4]
        match = sum(1 for w in yccd_words if w in text_lower)
        if (match / len(yccd_words) if yccd_words else 0) > 0.4: matched_ids.append(row)
    
    if not matched_ids and subj_info["default_id"]:
        defs = df[df['Id'] == subj_info["default_id"]]
        if not defs.empty: matched_ids.append(defs.iloc[0])

    # K·∫æT QU·∫¢
    final_results = []
    seen = set()
    for row in matched_ids[:2]:
        if row['Id'] in seen: continue
        seen.add(row['Id'])

        # --- G·ªåI C·∫¢ 2 AI ---
        gemini_res = ask_gemini(text, subject, row['YCCD'])
        grok_res = ask_grok(text, subject, row['YCCD'])
        
        # T·∫°o n·ªôi dung hi·ªÉn th·ªã
        exp_content = ""
        
        if gemini_res:
            exp_content += f"‚ú® **G·ª£i √Ω t·ª´ Gemini:**\n{gemini_res}\n\n"
        
        if grok_res:
            exp_content += f"üöÄ **G·ª£i √Ω t·ª´ Grok:**\n{grok_res}\n\n"
            
        if not gemini_res and not grok_res:
            exp_content = f"H·ªçc sinh s·ª≠ d·ª•ng c√¥ng c·ª• ({', '.join(found_tools)}) ƒë·ªÉ t·∫°o s·∫£n ph·∫©m s·ªë ph√π h·ª£p."

        final_results.append({
            "id": row['Id'],
            "yccd": row['YCCD'],
            "exp": exp_content
        })
    return final_results

# --- 6. GIAO DI·ªÜN ---
st.title("ü§ñ Gi√°o √Ån NLS (Gemini + Grok)")
st.caption("K·∫øt h·ª£p s·ª©c m·∫°nh ph√¢n t√≠ch t·ª´ Google v√† xAI.")
st.markdown("---")

col1, col2 = st.columns(2)
grade = col1.selectbox("Kh·ªëi l·ªõp", ["L·ªõp 6", "L·ªõp 7", "L·ªõp 8", "L·ªõp 9"])
subject = col2.selectbox("M√¥n h·ªçc", list(SUBJECT_MAPPING.keys()))
uploaded_file = st.file_uploader("T·∫£i gi√°o √°n", type=['docx', 'pdf'])

if uploaded_file and st.button("PH√ÇN T√çCH"):
    target = 'TC1' if grade in ['L·ªõp 6', 'L·ªõp 7'] else 'TC2'
    with st.spinner("ƒêang tham kh·∫£o √Ω ki·∫øn chuy√™n gia AI..."):
        content = read_file(uploaded_file)
        if len(content) < 50: st.warning("Kh√¥ng t√¨m th·∫•y NƒÉng l·ª±c s·ªë")
        else:
            df = load_nls_data()
            res = analyze_mixed_ai(content, df[df['Muc'] == target], subject)
            
            st.divider()
            if res:
                st.success(f"‚úÖ T√¨m th·∫•y {len(res)} ƒë·ªÅ xu·∫•t!")
                for item in res:
                    with st.expander(f"üìå M√£: {item['id']}", expanded=True):
                        st.code(f"{item['id']}: {item['yccd']}", language="text")
                        st.info(item['exp'])
            else:
                st.warning("Kh√¥ng t√¨m th·∫•y NƒÉng l·ª±c s·ªë cho b√†i h·ªçc n√†y")