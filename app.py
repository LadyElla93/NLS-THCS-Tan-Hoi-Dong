import streamlit as st
import pandas as pd
import docx2txt
import pdfplumber
import re

# --- Cáº¤U HÃŒNH TRANG ---
st.set_page_config(page_title="Trá»£ lÃ½ GiÃ¡o Ãn NLS Äa MÃ´n", page_icon="ğŸ“")

# --- 1. Tá»ª ÄIá»‚N Dá»® LIá»†U ÄA MÃ”N Há»ŒC ---
SUBJECT_MAPPING = {
    "ToÃ¡n há»c": {
        "keywords": ["geogebra", "mÃ¡y tÃ­nh cáº§m tay", "excel", "báº£ng tÃ­nh", "Ä‘á»“ thá»‹", "mÃ´ phá»ng", "tÃ­nh toÃ¡n"],
        "default_id": "5.1TC1a", "action": "giáº£i quyáº¿t váº¥n Ä‘á» tÃ­nh toÃ¡n"
    },
    "Ngá»¯ vÄƒn": {
        "keywords": ["soáº¡n tháº£o", "word", "powerpoint", "slide", "trÃ¬nh chiáº¿u", "video", "clip", "tra cá»©u", "e-book", "sÃ¢n kháº¥u hÃ³a"],
        "default_id": "3.1TC1a", "action": "sÃ¡ng táº¡o vÃ  trÃ¬nh bÃ y ná»™i dung"
    },
    "Tiáº¿ng Anh (Ngoáº¡i ngá»¯)": {
        "keywords": ["tá»« Ä‘iá»ƒn online", "app", "duolingo", "file nghe", "audio", "video", "ghi Ã¢m", "phÃ¡t Ã¢m", "chat", "email"],
        "default_id": "2.1TC1a", "action": "giao tiáº¿p vÃ  tra cá»©u ngÃ´n ngá»¯"
    },
    "KHTN (LÃ½/HÃ³a/Sinh)": {
        "keywords": ["thÃ­ nghiá»‡m áº£o", "phet", "mÃ´ phá»ng", "video thÃ­ nghiá»‡m", "cáº£m biáº¿n", "sá»‘ liá»‡u", "kÃ­nh hiá»ƒn vi Ä‘iá»‡n tá»­"],
        "default_id": "1.2TC1a", "action": "quan sÃ¡t vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u khoa há»c"
    },
    "Lá»‹ch sá»­ & Äá»‹a lÃ½": {
        "keywords": ["báº£n Ä‘á»“ sá»‘", "google earth", "lÆ°á»£c Ä‘á»“", "tranh áº£nh", "gps", "tÆ° liá»‡u", "internet", "phim tÆ° liá»‡u"],
        "default_id": "1.1TC1a", "action": "khai thÃ¡c thÃ´ng tin vÃ  Ä‘á»‹a lÃ½ trá»±c quan"
    },
    "Tin há»c": {
        "keywords": ["láº­p trÃ¬nh", "code", "thuáº­t toÃ¡n", "mÃ¡y tÃ­nh", "pháº§n má»m", "internet", "bÃ n phÃ­m", "chuá»™t", "thÆ° má»¥c"],
        "default_id": "5.4TC1a", "action": "thao tÃ¡c vÃ  giáº£i quyáº¿t váº¥n Ä‘á» trÃªn mÃ¡y tÃ­nh"
    },
    "CÃ´ng nghá»‡": {
        "keywords": ["báº£n váº½ ká»¹ thuáº­t", "thiáº¿t káº¿", "cad", "mÃ´ hÃ¬nh", "video hÆ°á»›ng dáº«n", "quy trÃ¬nh", "smart home", "internet of things"],
        "default_id": "3.1TC1b", "action": "thiáº¿t káº¿ vÃ  tÃ¬m hiá»ƒu quy trÃ¬nh ká»¹ thuáº­t"
    },
    "HÄ Tráº£i nghiá»‡m, HN": {
        "keywords": ["kháº£o sÃ¡t", "google form", "canva", "poster", "video", "áº£nh", "thuyáº¿t trÃ¬nh", "káº¿ hoáº¡ch", "tÃ¬m hiá»ƒu nghá»"],
        "default_id": "2.2TC1a", "action": "há»£p tÃ¡c vÃ  chia sáº» thÃ´ng tin"
    },
    "Nghá»‡ thuáº­t (Ã‚m nháº¡c/Má»¹ thuáº­t)": {
        "keywords": ["pháº§n má»m váº½", "chá»‰nh sá»­a áº£nh", "video", "ghi Ã¢m", "nháº¡c cá»¥ áº£o", "triá»ƒn lÃ£m áº£o", "file nháº¡c", "karaoke"],
        "default_id": "3.1TC1a", "action": "sÃ¡ng táº¡o tÃ¡c pháº©m nghá»‡ thuáº­t sá»‘"
    },
    "GDTC (Thá»ƒ dá»¥c)": {
        "keywords": ["video ká»¹ thuáº­t", "Ä‘á»“ng há»“ báº¥m giá»", "nhá»‹p tim", "app sá»©c khá»e", "ghi hÃ¬nh", "xem láº¡i", "clip"],
        "default_id": "4.3TC1a", "action": "theo dÃµi sá»©c khá»e vÃ  chá»‰nh sá»­a Ä‘á»™ng tÃ¡c"
    }
}

# --- 2. HÃ€M LOAD Dá»® LIá»†U ---
@st.cache_data
def load_nls_data():
    data = {
        'Id': ['1.1TC1a', '1.1TC2b', '1.3TC1a', '2.1TC1a', '2.2TC1a', '3.1TC1a', '3.1TC1b', '4.3TC1a', '5.1TC1a', '5.4TC1a'],
        'Muc': ['TC1', 'TC2', 'TC1', 'TC1', 'TC1', 'TC1', 'TC1', 'TC1', 'TC1', 'TC1'],
        'YCCD': [
            'XÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c nhu cáº§u thÃ´ng tin; tÃ¬m kiáº¿m dá»¯ liá»‡u trÃªn mÃ´i trÆ°á»ng sá»‘.',
            'Tá»• chá»©c tÃ¬m kiáº¿m thÃ´ng tin nÃ¢ng cao vÃ  Ä‘Ã¡nh giÃ¡ nguá»“n tin.',
            'Lá»±a chá»n vÃ  lÆ°u trá»¯ dá»¯ liá»‡u khoa há»c Ä‘á»ƒ truy xuáº¥t láº¡i sau nÃ y.',
            'Sá»­ dá»¥ng cÃ´ng nghá»‡ Ä‘á»ƒ tÆ°Æ¡ng tÃ¡c vÃ  giao tiáº¿p phÃ¹ há»£p vá»›i bá»‘i cáº£nh.',
            'Chia sáº» thÃ´ng tin vÃ  phá»‘i há»£p vá»›i ngÆ°á»i khÃ¡c qua cÃ´ng cá»¥ sá»‘.',
            'Táº¡o vÃ  chá»‰nh sá»­a ná»™i dung sá»‘ (vÄƒn báº£n, hÃ¬nh áº£nh, Ã¢m thanh).',
            'Thá»ƒ hiá»‡n báº£n thÃ¢n thÃ´ng qua viá»‡c táº¡o ra cÃ¡c sáº£n pháº©m sá»‘ Ä‘Æ¡n giáº£n.',
            'Sá»­ dá»¥ng cÃ´ng nghá»‡ Ä‘á»ƒ báº£o vá»‡ sá»©c khá»e vÃ  an toÃ n cÃ¡ nhÃ¢n.',
            'XÃ¡c Ä‘á»‹nh vÃ  giáº£i quyáº¿t cÃ¡c váº¥n Ä‘á» ká»¹ thuáº­t Ä‘Æ¡n giáº£n khi dÃ¹ng thiáº¿t bá»‹.',
            'Chá»§ Ä‘á»™ng tÃ¬m kiáº¿m cÆ¡ há»™i há»c táº­p vÃ  cáº­p nháº­t kiáº¿n thá»©c sá»‘.'
        ]
    }
    df = pd.DataFrame(data)
    df_tc2 = df.copy()
    df_tc2['Muc'] = 'TC2'
    return pd.concat([df, df_tc2])

# --- 3. HÃ€M Äá»ŒC FILE ---
def read_file(uploaded_file):
    try:
        if uploaded_file.name.endswith('.docx'):
            return docx2txt.process(uploaded_file)
        elif uploaded_file.name.endswith('.pdf'):
            text = ""
            with pdfplumber.open(uploaded_file) as pdf:
                for page in pdf.pages:
                    extract = page.extract_text()
                    if extract: text += extract + "\n"
            return text
    except:
        return ""
    return ""

# --- 4. THUáº¬T TOÃN PHÃ‚N TÃCH ---
def analyze_lesson_optimized(text, df, subject):
    results = []
    text_lower = text.lower()
    
    subj_info = SUBJECT_MAPPING.get(subject, {"keywords": [], "default_id": "", "action": "sá»­ dá»¥ng cÃ´ng nghá»‡"})
    subj_keywords = subj_info["keywords"]
    
    # 1. QuÃ©t cÃ´ng cá»¥
    found_tools = [kw for kw in subj_keywords if kw in text_lower]
    if not found_tools:
        common_digital_words = ["video", "trÃ¬nh chiáº¿u", "internet", "mÃ¡y tÃ­nh", "Ä‘iá»‡n thoáº¡i", "pháº§n má»m", "link", "web"]
        found_tools = [kw for kw in common_digital_words if kw in text_lower]

    # Náº¿u khÃ´ng tÃ¬m tháº¥y cÃ´ng cá»¥ nÃ o -> Tráº£ vá» danh sÃ¡ch rá»—ng ngay
    if not found_tools:
        return []

    # 2. QuÃ©t mÃ£ NLS
    matched_ids = []
    for _, row in df.iterrows():
        yccd_words = [w for w in row['YCCD'].lower().split() if len(w) > 4]
        match_count = sum(1 for w in yccd_words if w in text_lower)
        score = match_count / len(yccd_words) if yccd_words else 0
        if score > 0.4:
            matched_ids.append(row)

    # 3. Fallback
    if not matched_ids and subj_info["default_id"]:
        default_row = df[df['Id'] == subj_info["default_id"]]
        if not default_row.empty:
            matched_ids.append(default_row.iloc[0])

    # 4. Táº¡o káº¿t quáº£
    final_results = []
    seen_ids = set()

    for row in matched_ids:
        if row['Id'] in seen_ids: continue
        seen_ids.add(row['Id'])

        segments = re.split(r'(Hoáº¡t Ä‘á»™ng\s+[0-9]+|Pháº§n\s+[0-9]+|Luyá»‡n táº­p|Váº­n dá»¥ng|Khá»Ÿi Ä‘á»™ng)', text, flags=re.IGNORECASE)
        location = "Tiáº¿n trÃ¬nh dáº¡y há»c"
        for seg in segments:
            if len(seg) > 50 and any(t in seg.lower() for t in found_tools):
                location = "Hoáº¡t Ä‘á»™ng há»c táº­p cÃ³ sá»­ dá»¥ng thiáº¿t bá»‹/há»c liá»‡u sá»‘"
                break
        
        tool_str = ", ".join(found_tools[:3]) if found_tools else "thiáº¿t bá»‹ dáº¡y há»c"
        explanation = (
            f"Trong bÃ i há»c, há»c sinh Ä‘Æ°á»£c tiáº¿p cáº­n/sá»­ dá»¥ng: **{tool_str}**.\n"
            f"âœ… **Há»c sinh lÃ m Ä‘Æ°á»£c gÃ¬?** ThÃ´ng qua viá»‡c sá»­ dá»¥ng cÃ´ng cá»¥ nÃ y Ä‘á»ƒ {subj_info['action']}, "
            f"há»c sinh thá»±c hÃ nh Ä‘Æ°á»£c ká»¹ nÄƒng '{row['YCCD']}'. \n"
            f"Äiá»u nÃ y giÃºp chuyá»ƒn hÃ³a kiáº¿n thá»©c {subject} thÃ nh nÄƒng lá»±c thá»±c táº¿."
        )

        final_results.append({
            "id": row['Id'],
            "yccd": row['YCCD'],
            "loc": location,
            "exp": explanation
        })
    
    return final_results[:3]

# --- 5. GIAO DIá»†N CHÃNH ---
st.title("ğŸ¤– Trá»£ lÃ½ GiÃ¡o Ãn NLS (Äa MÃ´n)")
st.caption("Há»— trá»£: ToÃ¡n, VÄƒn, Anh, KHTN, Sá»­-Äá»‹a, Tin, CÃ´ng nghá»‡, HÄTN, Nghá»‡ thuáº­t, GDTC")
st.markdown("---")

col1, col2 = st.columns(2)
with col1:
    grade = st.selectbox("1. Khá»‘i lá»›p", ["Lá»›p 6", "Lá»›p 7", "Lá»›p 8", "Lá»›p 9"])
with col2:
    subject = st.selectbox("2. MÃ´n há»c", list(SUBJECT_MAPPING.keys()))

uploaded_file = st.file_uploader("3. Táº£i lÃªn giÃ¡o Ã¡n (Word/PDF)", type=['docx', 'pdf'])

if uploaded_file and st.button("ğŸš€ PHÃ‚N TÃCH NGAY"):
    target_muc = 'TC1' if grade in ['Lá»›p 6', 'Lá»›p 7'] else 'TC2'
    
    with st.spinner(f"Äang Ä‘á»c giÃ¡o Ã¡n mÃ´n {subject} - {grade}..."):
        content = read_file(uploaded_file)
        
        if len(content) < 50:
             # TrÆ°á»ng há»£p file lá»—i hoáº·c trá»‘ng
             st.warning("KhÃ´ng tÃ¬m tháº¥y NÄƒng lá»±c sá»‘ cho bÃ i há»c nÃ y")
        else:
            df_nls = load_nls_data()
            df_target = df_nls[df_nls['Muc'] == target_muc]
            
            findings = analyze_lesson_optimized(content, df_target, subject)
            
            st.divider()
            
            # --- PHáº¦N CHá»ˆNH Sá»¬A THEO YÃŠU Cáº¦U Má»šI ---
            if findings:
                st.success(f"âœ… ÄÃ£ tÃ¬m tháº¥y {len(findings)} nÄƒng lá»±c sá»‘ phÃ¹ há»£p!")
                for item in findings:
                    with st.expander(f"ğŸ“Œ MÃ£: {item['id']} (Chi tiáº¿t)", expanded=True):
                        st.markdown("**1. YÃªu cáº§u cáº§n Ä‘áº¡t (Má»¥c tiÃªu):**")
                        st.code(f"{item['id']}: {item['yccd']}", language="text")
                        st.markdown("**2. Giáº£i thÃ­ch hoáº¡t Ä‘á»™ng (Tiáº¿n trÃ¬nh):**")
                        st.info(f"{item['exp']}")
            else:
                # CHá»ˆ HIá»‚N THá»Š ÄÃšNG DÃ’NG NÃ€Y, KHÃ”NG NÃ“I GÃŒ THÃŠM
                st.warning("KhÃ´ng tÃ¬m tháº¥y NÄƒng lá»±c sá»‘ cho bÃ i há»c nÃ y")