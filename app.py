import streamlit as st
import pandas as pd
import docx2txt
import pdfplumber
import re

# --- 1. C·∫§U H√åNH T·ª™ ƒêI·ªÇN ƒê·ªÜM CHO C√ÅC M√îN H·ªåC ---
# ƒê√¢y l√† "b·ªô n√£o" gi√∫p App hi·ªÉu ti·∫øng n√≥i c·ªßa gi√°o vi√™n t·ª´ng m√¥n
SUBJECT_MAPPING = {
    "To√°n h·ªçc": {
        "keywords": ["geogebra", "m√°y t√≠nh c·∫ßm tay", "ƒë·ªì th·ªã", "t√≠nh to√°n", "excel", "m√¥ ph·ªèng", "s·ªë li·ªáu"],
        "nls_id_suggest": ["5.1TC1a", "5.1TC2b"] # G·ª£i √Ω m√£ NLS th∆∞·ªùng g·∫∑p (Gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ)
    },
    "Ng·ªØ vƒÉn": {
        "keywords": ["so·∫°n th·∫£o", "vƒÉn b·∫£n", "tr√¨nh chi·∫øu", "clip", "video", "tra c·ª©u", "t√°c gi·∫£", "e-book"],
        "nls_id_suggest": ["3.1TC1a", "1.1TC1a"] # G·ª£i √Ω m√£ NLS th∆∞·ªùng g·∫∑p (S√°ng t·∫°o n·ªôi dung, T√¨m tin)
    },
    "L·ªãch s·ª≠ & ƒê·ªãa l√Ω": {
        "keywords": ["b·∫£n ƒë·ªì s·ªë", "google earth", "l∆∞·ª£c ƒë·ªì", "t∆∞ li·ªáu", "tranh ·∫£nh", "gps", "t√¨m ngu·ªìn"],
        "nls_id_suggest": ["1.2TC1a", "1.1TC1b"] # (ƒê√°nh gi√° th√¥ng tin, T√¨m ki·∫øm)
    },
    "Khoa h·ªçc t·ª± nhi√™n (L√Ω/H√≥a/Sinh)": {
        "keywords": ["th√≠ nghi·ªám ·∫£o", "m√¥ ph·ªèng", "phet", "video th√≠ nghi·ªám", "ghi l·∫°i s·ªë li·ªáu", "c·∫£m bi·∫øn"],
        "nls_id_suggest": ["5.3TC1a", "1.3TC1a"] # (S·ª≠ d·ª•ng s√°ng t·∫°o, Qu·∫£n l√Ω d·ªØ li·ªáu)
    },
    "Tin h·ªçc": {
        "keywords": [], # Tin h·ªçc th√¨ d√πng ch√≠nh t·ª´ kh√≥a g·ªëc c·ªßa NLS
        "nls_id_suggest": []
    }
}

# --- 2. H√ÄM T·∫¢I D·ªÆ LI·ªÜU NLS (GI·ªÆ NGUY√äN) ---
@st.cache_data
def load_nls_data():
    # Trong th·ª±c t·∫ø b·∫°n load file CSV ƒë·∫ßy ƒë·ªß
    data = {
        'Id': ['1.1TC1a', '1.1TC2b', '3.1TC1a', '5.1TC1a'],
        'Muc': ['TC1', 'TC2', 'TC1', 'TC1'],
        'YCCD': [
            'X√°c ƒë·ªãnh ƒë∆∞·ª£c nhu c·∫ßu th√¥ng tin; t√¨m ki·∫øm d·ªØ li·ªáu tr√™n m·∫°ng',
            'T·ªï ch·ª©c t√¨m ki·∫øm th√¥ng tin n√¢ng cao',
            'T·∫°o v√† ch·ªânh s·ª≠a n·ªôi dung s·ªë (vƒÉn b·∫£n, h√¨nh ·∫£nh)',
            'Gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ k·ªπ thu·∫≠t khi v·∫≠n h√†nh thi·∫øt b·ªã'
        ]
    }
    return pd.DataFrame(data)

# --- 3. LOGIC PH√ÇN T√çCH N√ÇNG CAO ---
def analyze_content_advanced(lesson_text, df_nls, subject_choice):
    results = []
    lesson_text_lower = lesson_text.lower()
    
    # L·∫•y danh s√°ch t·ª´ kh√≥a b·ªï sung c·ªßa m√¥n h·ªçc ƒë√≥
    extra_keywords = []
    if subject_choice in SUBJECT_MAPPING:
        extra_keywords = SUBJECT_MAPPING[subject_choice]['keywords']
    
    for index, row in df_nls.iterrows():
        # 1. T·ª´ kh√≥a g·ªëc c·ªßa NLS
        original_keywords = [w for w in row['YCCD'].lower().split() if len(w) > 3]
        
        # 2. K·∫øt h·ª£p t·ª´ kh√≥a m√¥n h·ªçc v√†o vi·ªác t√¨m ki·∫øm
        # N·∫øu gi√°o √°n ch·ª©a t·ª´ kh√≥a m√¥n h·ªçc (VD: "GeoGebra") V√Ä ID n√†y thu·ªôc nh√≥m g·ª£i √Ω -> TƒÉng ƒëi·ªÉm kh·ªõp
        bonus_score = 0
        
        # Ki·ªÉm tra t·ª´ kh√≥a m√¥n h·ªçc xu·∫•t hi·ªán trong b√†i kh√¥ng
        found_subject_kw = [kw for kw in extra_keywords if kw in lesson_text_lower]
        
        if found_subject_kw:
            # N·∫øu t√¨m th·∫•y t·ª´ kh√≥a chuy√™n ng√†nh, ta xem x√©t ID n√†y c√≥ li√™n quan kh√¥ng
            # ƒê√¢y l√† logic "m·ªü r·ªông": Map t·ª´ kh√≥a chuy√™n ng√†nh sang NLS
            bonus_score = 0.3 # C·ªông 30% ƒë·ªô tin c·∫≠y
        
        # ƒê·∫øm t·ª´ kh√≥a g·ªëc
        match_count = sum(1 for word in original_keywords if word in lesson_text_lower)
        base_score = match_count / len(original_keywords) if original_keywords else 0
        
        final_score = base_score + bonus_score
        
        # Ng∆∞·ª°ng duy·ªát (th·∫•p h∆°n m·ªôt ch√∫t v√¨ ƒë√£ c√≥ bonus)
        if final_score > 0.5:
            # T√¨m v·ªã tr√≠ (Logic b∆∞·ªõc 4)
            segments = re.split(r'(Ho·∫°t ƒë·ªông\s+[0-9]+|Ph·∫ßn\s+[0-9]+)', lesson_text, flags=re.IGNORECASE)
            location = "Ti·∫øn tr√¨nh d·∫°y h·ªçc"
            for seg in segments:
                if len(seg) > 50 and (any(k in seg.lower() for k in original_keywords) or any(k in seg.lower() for k in found_subject_kw)):
                    # L·∫•y t√™n ho·∫°t ƒë·ªông tr∆∞·ªõc ƒë√≥ (gi·∫£ l·∫≠p)
                    location = "Ho·∫°t ƒë·ªông h·ªçc t·∫≠p (c√≥ ch·ª©a t·ª´ kh√≥a li√™n quan)"
                    break
            
            # T·∫°o l·ªùi gi·∫£i th√≠ch theo m√¥n
            explanation = (
                f"Trong m√¥n {subject_choice}, vi·ªác s·ª≠ d·ª•ng c√°c y·∫øu t·ªë nh∆∞ {', '.join(found_subject_kw) if found_subject_kw else 'c√¥ng ngh·ªá s·ªë'} "
                f"ƒë√°p ·ª©ng y√™u c·∫ßu '{row['YCCD']}'. "
                f"Gi√∫p h·ªçc sinh kh√¥ng ch·ªâ h·ªçc ki·∫øn th·ª©c {subject_choice} m√† c√≤n ph√°t tri·ªÉn k·ªπ nƒÉng s·ªë."
            )

            results.append({
                "ID": row['Id'],
                "YCCD": row['YCCD'],
                "Vi_tri": location,
                "Giai_thich": explanation
            })
            
    return results

# --- 4. GIAO DI·ªÜN STREAMLIT ---
st.title("ü§ñ Tr·ª£ l√Ω AI So·∫°n Gi√°o √Ån T√≠ch H·ª£p NLS")
st.markdown("---")

col1, col2 = st.columns(2)
with col1:
    grade_option = st.selectbox("1. Ch·ªçn Kh·ªëi l·ªõp", ('L·ªõp 6', 'L·ªõp 7', 'L·ªõp 8', 'L·ªõp 9'))
with col2:
    # TH√äM: Ch·ªçn m√¥n h·ªçc ƒë·ªÉ t·ªëi ∆∞u thu·∫≠t to√°n
    subject_option = st.selectbox("2. Ch·ªçn M√¥n h·ªçc", 
                                  ("To√°n h·ªçc", "Ng·ªØ vƒÉn", "L·ªãch s·ª≠ & ƒê·ªãa l√Ω", "Khoa h·ªçc t·ª± nhi√™n (L√Ω/H√≥a/Sinh)", "Tin h·ªçc", "Kh√°c"))

uploaded_file = st.file_uploader("3. T·∫£i l√™n gi√°o √°n (Word/PDF)", type=['docx', 'pdf'])

if uploaded_file:
    # X·ª≠ l√Ω mapping l·ªõp
    target_muc = 'TC1' if grade_option in ['L·ªõp 6', 'L·ªõp 7'] else 'TC2'
    
    # ƒê·ªçc file
    text_content = ""
    if uploaded_file.name.endswith('.docx'):
        text_content = docx2txt.process(uploaded_file)
    # (Ph·∫ßn PDF gi·ªØ nguy√™n nh∆∞ c≈©)

    # Load data v√† l·ªçc
    df = load_nls_data()
    df_filtered = df[df['Muc'] == target_muc]

    st.info(f"ƒêang ph√¢n t√≠ch gi√°o √°n m√¥n **{subject_option}** - Kh·ªëi **{target_muc}**...")
    
    # G·ªåI H√ÄM PH√ÇN T√çCH N√ÇNG CAO
    analysis_results = analyze_content_advanced(text_content, df_filtered, subject_option)

    st.divider()
    if analysis_results:
        st.success(f"‚úÖ T√¨m th·∫•y {len(analysis_results)} ƒëi·ªÉm t√≠ch h·ª£p ph√π h·ª£p!")
        
        for item in analysis_results:
            with st.expander(f"üìå {item['ID']} - Click ƒë·ªÉ xem chi ti·∫øt"):
                st.markdown("**1. Y√™u c·∫ßu c·∫ßn ƒë·∫°t (Copy v√†o M·ª•c ti√™u):**")
                st.code(f"{item['ID']}: {item['YCCD']}", language='text')
                
                st.markdown("**2. Gi·∫£i th√≠ch s∆∞ ph·∫°m (Copy v√†o Gi√°o √°n):**")
                st.info(item['Giai_thich'])
    else:
        st.warning(f"Ch∆∞a t√¨m th·∫•y s·ª± t∆∞∆°ng ƒë·ªìng r√µ r·ªát. H√£y th·ª≠ th√™m c√°c t·ª´ kh√≥a c√¥ng ngh·ªá (v√≠ d·ª•: ph·∫ßn m·ªÅm, internet, video...) v√†o gi√°o √°n m√¥n {subject_option} c·ªßa b·∫°n.")