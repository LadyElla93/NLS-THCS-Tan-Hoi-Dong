import streamlit as st
import pandas as pd
import docx2txt
import pdfplumber
import re

# --- C·∫§U H√åNH TRANG (B·∫ÆT BU·ªòC PH·∫¢I ·ªû D√íNG ƒê·∫¶U TI√äN) ---
st.set_page_config(page_title="Tr·ª£ l√Ω NLS (Deep Scan)", page_icon="üéØ", layout="centered")

# --- KH·ªêI B·∫ÆT L·ªñI TO√ÄN C·ª§C ---
try:
    # --- 1. T·ª™ ƒêI·ªÇN D·ªÆ LI·ªÜU ---
    SUBJECT_DATA = {
        "To√°n h·ªçc": {"kw": ["geogebra", "m√°y t√≠nh", "excel", "ƒë·ªì th·ªã", "t√≠nh to√°n"], "id": "5.1TC1a", "prod": "H√¨nh v·∫Ω ƒë·ªì th·ªã ho·∫∑c K·∫øt qu·∫£ t√≠nh to√°n s·ªë"},
        "Ng·ªØ vƒÉn": {"kw": ["so·∫°n th·∫£o", "word", "powerpoint", "tr√¨nh chi·∫øu", "video", "tra c·ª©u"], "id": "3.1TC1a", "prod": "Slide thuy·∫øt tr√¨nh ho·∫∑c VƒÉn b·∫£n s·ªë h√≥a"},
        "Ti·∫øng Anh": {"kw": ["t·ª´ ƒëi·ªÉn", "file nghe", "audio", "video", "ghi √¢m", "app"], "id": "2.1TC1a", "prod": "File ghi √¢m ho·∫∑c H·ªôi tho·∫°i s·ªë"},
        "KHTN": {"kw": ["th√≠ nghi·ªám ·∫£o", "m√¥ ph·ªèng", "s·ªë li·ªáu", "k√≠nh hi·ªÉn vi", "video"], "id": "1.2TC1a", "prod": "B·∫£ng s·ªë li·ªáu ho·∫∑c Video th√≠ nghi·ªám"},
        "L·ªãch s·ª≠ & ƒê·ªãa l√Ω": {"kw": ["b·∫£n ƒë·ªì", "google earth", "tranh ·∫£nh", "gps", "t∆∞ li·ªáu"], "id": "1.1TC1a", "prod": "B·∫£n ƒë·ªì s·ªë ho·∫∑c B·ªô s∆∞u t·∫≠p t∆∞ li·ªáu"},
        "Tin h·ªçc": {"kw": ["l·∫≠p tr√¨nh", "code", "thu·∫≠t to√°n", "m√°y t√≠nh", "ph·∫ßn m·ªÅm"], "id": "5.4TC1a", "prod": "Ch∆∞∆°ng tr√¨nh m√°y t√≠nh"},
        "C√¥ng ngh·ªá": {"kw": ["b·∫£n v·∫Ω", "thi·∫øt k·∫ø", "cad", "m√¥ h√¨nh", "video"], "id": "3.1TC1b", "prod": "B·∫£n thi·∫øt k·∫ø k·ªπ thu·∫≠t s·ªë"},
        "Hƒê Tr·∫£i nghi·ªám": {"kw": ["kh·∫£o s√°t", "form", "canva", "poster", "video", "·∫£nh"], "id": "2.2TC1a", "prod": "Poster truy·ªÅn th√¥ng s·ªë"},
        "Ngh·ªá thu·∫≠t": {"kw": ["v·∫Ω", "ch·ªânh ·∫£nh", "video", "ghi √¢m", "nh·∫°c c·ª•"], "id": "3.1TC1a", "prod": "T√°c ph·∫©m ngh·ªá thu·∫≠t s·ªë"},
        "GDTC": {"kw": ["video", "ƒë·ªìng h·ªì", "nh·ªãp tim", "app", "ghi h√¨nh"], "id": "4.3TC1a", "prod": "Video ph√¢n t√≠ch ƒë·ªông t√°c"}
    }

    # --- 2. DATA NLS C·ªêT L√ïI ---
    @st.cache_data
    def load_nls_db():
        data = [
            {"Id": "1.1TC1a", "Muc": "TC1", "YCCD": "X√°c ƒë·ªãnh nhu c·∫ßu v√† t√¨m ki·∫øm d·ªØ li·ªáu tr√™n m√¥i tr∆∞·ªùng s·ªë."},
            {"Id": "1.2TC1a", "Muc": "TC1", "YCCD": "Ph√¢n t√≠ch v√† ƒë√°nh gi√° d·ªØ li·ªáu, th√¥ng tin s·ªë."},
            {"Id": "2.1TC1a", "Muc": "TC1", "YCCD": "S·ª≠ d·ª•ng c√¥ng ngh·ªá ƒë·ªÉ t∆∞∆°ng t√°c v√† giao ti·∫øp ph√π h·ª£p."},
            {"Id": "2.2TC1a", "Muc": "TC1", "YCCD": "Chia s·∫ª th√¥ng tin v√† ph·ªëi h·ª£p qua m√¥i tr∆∞·ªùng s·ªë."},
            {"Id": "3.1TC1a", "Muc": "TC1", "YCCD": "T·∫°o v√† bi√™n t·∫≠p n·ªôi dung s·ªë (vƒÉn b·∫£n, h√¨nh ·∫£nh)."},
            {"Id": "3.1TC1b", "Muc": "TC1", "YCCD": "Th·ªÉ hi·ªán b·∫£n th√¢n qua s·∫£n ph·∫©m s·ªë ƒë∆°n gi·∫£n."},
            {"Id": "4.3TC1a", "Muc": "TC1", "YCCD": "S·ª≠ d·ª•ng thi·∫øt b·ªã s·ªë an to√†n, b·∫£o v·ªá s·ª©c kh·ªèe."},
            {"Id": "5.1TC1a", "Muc": "TC1", "YCCD": "Gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ k·ªπ thu·∫≠t c∆° b·∫£n c·ªßa thi·∫øt b·ªã s·ªë."},
            {"Id": "5.4TC1a", "Muc": "TC1", "YCCD": "T·ª± c·∫≠p nh·∫≠t v√† ph√°t tri·ªÉn nƒÉng l·ª±c s·ªë b·∫£n th√¢n."}
        ]
        # Nh√¢n b·∫£n cho TC2
        full_data = []
        for item in data:
            full_data.append(item)
            item2 = item.copy()
            item2["Muc"] = "TC2"
            full_data.append(item2)
        return pd.DataFrame(full_data)

    # --- 3. THU·∫¨T TO√ÅN ƒê·ªåC S√ÇU (DEEP SCAN) ---
    def analyze_deep(text, subject, grade):
        results = []
        # X·ª≠ l√Ω text an to√†n
        if not text: return []
        
        text_lower = text.lower()
        subj_config = SUBJECT_DATA.get(subject, SUBJECT_DATA["To√°n h·ªçc"])
        keywords = subj_config["kw"]
        
        # C·∫Øt ƒëo·∫°n vƒÉn th√¥ng minh
        segments = re.split(r'(Ho·∫°t ƒë·ªông\s+\d+|II\.|III\.|Ti·∫øn tr√¨nh|Luy·ªán t·∫≠p|V·∫≠n d·ª•ng)', text)
        current_loc = "N·ªôi dung b√†i h·ªçc"
        
        for segment in segments:
            # X√°c ƒë·ªãnh ti√™u ƒë·ªÅ
            if len(segment) < 60 and len(segment) > 3 and any(x in segment for x in ["Ho·∫°t ƒë·ªông", "II.", "III.", "Ti·∫øn tr√¨nh"]):
                current_loc = segment.strip()
                continue
            
            # Qu√©t n·ªôi dung
            if len(segment) > 30:
                found_kws = [k for k in keywords if k in segment.lower()]
                if found_kws:
                    # Tr√≠ch xu·∫•t c√¢u ch·ª©ng minh
                    sentences = segment.split('.')
                    evidence = next((s for s in sentences if any(k in s.lower() for k in found_kws)), f"S·ª≠ d·ª•ng {found_kws[0]}")
                    
                    # T√¨m d·ªØ li·ªáu NLS
                    target_muc = 'TC1' if grade in ['L·ªõp 6', 'L·ªõp 7'] else 'TC2'
                    df = load_nls_db()
                    row = df[(df['Id'] == subj_config['id']) & (df['Muc'] == target_muc)]
                    if row.empty: row = df[df['Muc'] == target_muc].iloc[0]
                    else: row = row.iloc[0]

                    results.append({
                        "vitri": current_loc,
                        "id": row['Id'],
                        "yccd": row['YCCD'],
                        "tool": found_kws[0],
                        "prod": subj_config['prod'],
                        "evidence": evidence.strip()
                    })
                    if len(results) >= 2: break # Ch·ªâ l·∫•y t·ªëi ƒëa 2 k·∫øt qu·∫£ t·ªët nh·∫•t
        return results

    # --- 4. GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG ---
    st.title("üéØ Tr·ª£ L√Ω So√°t Gi√°o √Ån (Deep Scan)")
    st.info("H·ªá th·ªëng t·ª± ƒë·ªông ƒë·ªçc n·ªôi dung v√† ƒë·ªÅ xu·∫•t v·ªã tr√≠ ch√®n NLS ch√≠nh x√°c.")
    st.markdown("---")

    c1, c2 = st.columns(2)
    grade = c1.selectbox("Kh·ªëi l·ªõp", ["L·ªõp 6", "L·ªõp 7", "L·ªõp 8", "L·ªõp 9"])
    subject = c2.selectbox("M√¥n h·ªçc", list(SUBJECT_DATA.keys()))
    uploaded_file = st.file_uploader("T·∫£i gi√°o √°n (Word/PDF)", type=['docx', 'pdf'])

    if uploaded_file and st.button("QU√âT N·ªòI DUNG"):
        content = ""
        try:
            if uploaded_file.name.endswith('.docx'): content = docx2txt.process(uploaded_file)
            elif uploaded_file.name.endswith('.pdf'):
                with pdfplumber.open(uploaded_file) as pdf:
                    for p in pdf.pages: content += p.extract_text() + "\n"
        except Exception as e:
            st.error(f"L·ªói ƒë·ªçc file: {e}")

        if len(content) < 50:
            st.warning("File tr·ªëng ho·∫∑c kh√¥ng ƒë·ªçc ƒë∆∞·ª£c n·ªôi dung.")
        else:
            findings = analyze_deep(content, subject, grade)
            
            st.divider()
            if findings:
                st.success(f"‚úÖ T√¨m th·∫•y {len(findings)} v·ªã tr√≠ ph√π h·ª£p:")
                for i, item in enumerate(findings):
                    # Hi·ªÉn th·ªã k·∫øt qu·∫£ d·∫°ng th·∫ª (Card)
                    with st.container():
                        st.subheader(f"üìç {item['vitri']}")
                        st.markdown(f"> *\"{item['evidence']}...\"*")
                        
                        st.markdown(f"**ƒê·ªÅ xu·∫•t b·ªï sung:**")
                        insert_text = (
                            f"üëâ **Ho·∫°t ƒë·ªông:** S·ª≠ d·ª•ng **{item['tool']}** ƒë·ªÉ t·∫°o **{item['prod']}**.\n"
                            f"üëâ **ƒê√°p ·ª©ng YCCƒê:** [{item['id']}] {item['yccd']}"
                        )
                        st.info(insert_text)
                        st.markdown("---")
            else:
                st.warning("Kh√¥ng t√¨m th·∫•y n·ªôi dung t√≠ch h·ª£p NƒÉng L·ª±c S·ªë ph√π h·ª£p.")

except Exception as e:
    st.error("‚ö†Ô∏è ƒê√£ x·∫£y ra l·ªói h·ªá th·ªëng:")
    st.code(e)
    st.caption("H√£y ch·ª•p m√†n h√¨nh l·ªói n√†y g·ª≠i cho k·ªπ thu·∫≠t vi√™n.")